---
title: "Biostat 203B Homework 4"
subtitle: "Due Mar 9 @ 11:59PM"
author: "Khoa Vu 705600710"
format:
  html:
    theme: cosmo
    embed-resources: true
    number-sections: false
    toc: true
    toc-depth: 4
    toc-location: left
    code-fold: false
---

Display machine information:
```{r}
sessionInfo()
```
Display my machine memory.
```{r}
memuse::Sys.meminfo()
```

Load database libraries and the tidyverse frontend:
```{r}
library(bigrquery)
library(dbplyr)
library(DBI)
library(gt)
library(gtsummary)
library(tidyverse)
```

## Q1. Compile the ICU cohort in HW3 from the Google BigQuery database 

Below is an outline of steps. In this homework, we exclusively work with the BigQuery database and should not use any MIMIC data files stored on our local computer. Transform data as much as possible in BigQuery database and `collect()` the tibble **only at the end of Q1.7**.

### Q1.1 Connect to BigQuery

Authenticate with BigQuery using the service account token. Please place the service account token (shared via BruinLearn) in the working directory (same folder as your qmd file). Do **not** ever add this token to your Git repository. If you do so, you will lose 50 points.
```{r}
# path to the service account token 
satoken <- "biostat-203b-2025-winter-4e58ec6e5579.json"
# BigQuery authentication using service account
bq_auth(path = satoken)
```
Connect to BigQuery database `mimiciv_3_1` in GCP (Google Cloud Platform), using the project billing account `biostat-203b-2025-winter`.
```{r}
# connect to the BigQuery database `biostat-203b-2025-mimiciv_3_1`
con_bq <- dbConnect(
    bigrquery::bigquery(),
    project = "biostat-203b-2025-winter",
    dataset = "mimiciv_3_1",
    billing = "biostat-203b-2025-winter"
)
con_bq
```
List all tables in the `mimiciv_3_1` database.
```{r}
dbListTables(con_bq)
```

### Q1.2 `icustays` data

Connect to the `icustays` table.
```{r}
# full ICU stays table
icustays_tble <- tbl(con_bq, "icustays") |>
  arrange(subject_id, hadm_id, stay_id) |>
  # show_query() |>
  print(width = Inf)
```

### Q1.3 `admissions` data

Connect to the `admissions` table.
```{r}
# # TODO
admissions_tble <- tbl(con_bq, "admissions") |>
  arrange(subject_id, hadm_id) |>
  # show_query() |>
  print(width = Inf)
```

### Q1.4 `patients` data

Connect to the `patients` table.
```{r}
# # TODO
patients_tble <- tbl(con_bq, "patients") |>
  arrange(subject_id) |>
  # show_query() |>
  print(width = Inf)
```

### Q1.5 `labevents` data

Connect to the `labevents` table and retrieve a subset that only contain subjects who appear in `icustays_tble` and the lab items listed in HW3. Only keep the last lab measurements (by `storetime`) before the ICU stay and pivot lab items to become variables/columns. Write all steps in _one_ chain of pipes.
```{r}
# # TODO
subset_itemid <- c(50912, 50971, 50983, 50902, 50882, 51221, 51301, 50931)

#Connecting to d_labitems to get lab item names
d_labitems_tble <- tbl(con_bq, "d_labitems") |>
  arrange(itemid) |>
  # show_query() |>
  print(width = Inf)

labevents_tble <- tbl(con_bq, "labevents") |>
  #Filtering by measurements we want to take
  filter(itemid %in% subset_itemid) |>
  #We keep only the columns we need
  select(subject_id, itemid, storetime, valuenum) |>
  #Mutating columns for proper joins
  #Mutate subject_id and hadm_id as double to join with icustays_tble
  mutate(subject_id = as.double(subject_id)) |>
  #We inner join with icu stays, keeping only the patients with an icu stay
  #We inner join to discard nonmatching rows from both tibbles
  inner_join(icustays_tble, by = c("subject_id"), copy = TRUE) |>
  filter(storetime < intime) |>
  #We group by stay_id and itemid to get the values for each measurement
  #of each stay 
  group_by(subject_id, stay_id, itemid) |>
  #We order by the store time, taking the last measurement before intime
  #Using slice_max(), we take a slice of size one for the highest time
  slice_max(order_by = storetime, n = 1) |>
  summarize(valuenum = mean(valuenum, na.rm = TRUE)) |>
  #We ungroup to get the dataframe back to a normal size
  ungroup() |>
  #We join labevents_tble with d_labitems by itemid
  left_join(d_labitems_tble, by = "itemid", copy = TRUE) |> 
  #We subset labevents_tbl to only the columns we need in our final result
  select(c(subject_id, stay_id, valuenum, label)) |>
  #Apply lower case to all labels
  mutate(label = tolower(label)) |>
  #We widen the dataframe to get each row as a subject and ICU stay
  pivot_wider(names_from = label, values_from = valuenum) |>
  #Sorting the tble by subject_id and_stay id for grading purposes
  arrange(subject_id, stay_id) |>
  #Changing white blood cells to wbc, removing spaces
  rename(wbc = `white blood cells`) |>
  # show_query() |>
  print(width = Inf)
  
```

### Q1.6 `chartevents` data

Connect to `chartevents` table and retrieve a subset that only contain subjects who appear in `icustays_tble` and the chart events listed in HW3. Only keep the first chart events (by `storetime`) during ICU stay and pivot chart events to become variables/columns. Write all steps in _one_ chain of pipes. Similary to HW3, if a vital has multiple measurements at the first `storetime`, average them.
```{r}
# # TODO
#Taking the subset of item_ids needed
subset_itemid <-c(220045, 220180, 220179, 223761, 220210)

#Connecting to d_items to get item names
d_items_tble <- tbl(con_bq, "d_items") |>
  arrange(itemid) |>
  # show_query() |>
  print(width = Inf)

chartevents_tble <- tbl(con_bq, "chartevents") |>
  #Subset based on needed measurements
  filter(itemid %in% subset_itemid) |>
  #We keep only the columns we need
  select(subject_id, stay_id, itemid, storetime, valuenum) |>
  #Mutate subject_id and stay_id as double to join with icustays_tble
  mutate(subject_id = as.double(subject_id)) |>
  mutate(stay_id = as.double(stay_id)) |>
  #Inner join with ICU Stays
  #Keep patients with an ICU stay
  inner_join(icustays_tble, by = c("subject_id", "stay_id"), copy = TRUE) |>
  #Filter by measurements within the ICU stay
  filter((storetime > intime) & (storetime < outtime)) |>
  #We group by stay_id and itemid to get the values for each measurement
  #of each stay 
  group_by(subject_id, stay_id, itemid) |>
  #We order by the store time, taking the first measurement during the ICU stay
  #Using slice_min(), we take a slice of size one for the smallest time in that
  #interval
  slice_min(order_by = storetime, n = 1) |>
  summarize(valuenum = mean(valuenum, na.rm = TRUE)) |>
  #We ungroup to get the dataframe back to a normal size
  ungroup() %>%
  left_join(d_items_tble, by = "itemid", copy = TRUE) |>
  select(c(subject_id, stay_id, valuenum, label)) |>
  #Apply lower case to all labels and remove sapce
  #Note, we have to use the SQL equivalent of str_replace_all
  #Which is REGEXP_REPLACE()
  mutate(label = REGEXP_REPLACE(tolower(label), " ", "_")) |>
  #We widen the dataframe to get each row as a subject and ICU stay
  pivot_wider(names_from = label, values_from = valuenum) |>
  #Sorting the tble by subject_id and_stay id for grading purposes
  arrange(subject_id, stay_id) |>
  # show_query() |>
  print(width = Inf)
```

### Q1.7 Put things together

This step is similar to Q7 of HW3. Using _one_ chain of pipes `|>` to perform following data wrangling steps: (i) start with the `icustays_tble`, (ii) merge in admissions and patients tables, (iii) keep adults only (age at ICU intime >= 18), (iv) merge in the labevents and chartevents tables, (v) `collect` the tibble, (vi) sort `subject_id`, `hadm_id`, `stay_id` and `print(width = Inf)`.

```{r}
# # TODO
mimic_icu_cohort <- icustays_tble |>
  #Left join with admissions_tble by subject_id and hadm_id
  left_join(admissions_tble, by = c("subject_id", "hadm_id")) |>
  #Left join with patients_tbl by subject_id
  left_join(patients_tble, by = c("subject_id")) |>
  #Left join with labevents_tble by subject_id and stay_id
  left_join(labevents_tble, by = c("subject_id", "stay_id")) |>
  #Left join with chartevents_tble by subject_id and stay_id
  left_join(chartevents_tble, by = c("subject_id", "stay_id")) |>
  #Creating age at intime %>%
  mutate(age_intime = anchor_age + (year(intime) - anchor_year)) |>
  #Filter by adults (age_intime >= 18)
  filter(age_intime >= 18) |>
  #Collecting the tibble
  collect() |>
  #Sorting the tble by subject_id and_stay id for grading purposes
  arrange(subject_id, stay_id) |>
  print(width = Inf)
```

### Q1.8 Preprocessing

Perform the following preprocessing steps. (i) Lump infrequent levels into "Other" level for `first_careunit`, `last_careunit`, `admission_type`, `admission_location`, and `discharge_location`. (ii) Collapse the levels of `race` into `ASIAN`, `BLACK`, `HISPANIC`, `WHITE`, and `Other`. (iii) Create a new variable `los_long` that is `TRUE` when `los` is greater than or equal to 2 days. (iv) Summarize the data using `tbl_summary()`, stratified by `los_long`. Hint: `fct_lump_n` and `fct_collapse` from the `forcats` package are useful.

Hint: Below is a numerical summary of my tibble after preprocessing:

<iframe width=95% height="500" src="./mimic_icu_cohort_gtsummary.html"></iframe>

**Solution**

```{r}
library('forcats')

mimic_icu_cohort <- mimic_icu_cohort |>
  #Lumping infrequent levels using fct_lump_n
  mutate(first_careunit = fct_lump_n(first_careunit, n = 4),
  last_careunit = fct_lump_n(last_careunit, n = 4),
  admission_type = fct_lump_n(admission_type, n = 4),
  admission_location = fct_lump_n(admission_location, n = 4),
  discharge_location = fct_lump_n(discharge_location, n = 4)) |>
  #Collapsing the levels of race
  mutate(race = fct_collapse(race, 
  ASIAN = unique(mimic_icu_cohort$race)[grep('ASIAN', 
  unique(mimic_icu_cohort$race))],
  BLACK = unique(mimic_icu_cohort$race)[grep('BLACK', 
  unique(mimic_icu_cohort$race))],
  HISPANIC = unique(mimic_icu_cohort$race)[grep('HISPANIC', 
  unique(mimic_icu_cohort$race))],
  WHITE = unique(mimic_icu_cohort$race)[grep('WHITE', 
  unique(mimic_icu_cohort$race))],
  OTHER = unique(mimic_icu_cohort$race)[!grepl('ASIAN|BLACK|HISPANIC|WHITE', 
  unique(mimic_icu_cohort$race))])) |>
  #Creating a variable los_long 
  mutate(los_long = (los >= 2)) |>
  tbl_summary(by = los_long, include = c(first_careunit, last_careunit, los,
  admission_type, admission_location, discharge_location, insurance, language,
  marital_status, race, hospital_expire_flag, gender, dod, chloride, creatinine,
  sodium, potassium, glucose, hematocrit, wbc, bicarbonate, 
  non_invasive_blood_pressure_systolic, non_invasive_blood_pressure_diastolic,
  respiratory_rate, temperature_fahrenheit, heart_rate, age_intime))

mimic_icu_cohort
```

### Q1.9 Save the final tibble

Save the final tibble to an R data file `mimic_icu_cohort.rds` in the `mimiciv_shiny` folder.
```{r}
# make a directory mimiciv_shiny
if (!dir.exists("mimiciv_shiny")) {
  dir.create("mimiciv_shiny")
}
# save the final tibble
mimic_icu_cohort |>
  write_rds("mimiciv_shiny/mimic_icu_cohort.rds", compress = "gz")
```
Close database connection and clear workspace.
```{r}
if (exists("con_bq")) {
  dbDisconnect(con_bq)
}
rm(list = ls())
```
Although it is not a good practice to add big data files to Git, for grading purpose, please add `mimic_icu_cohort.rds` to your Git repository.

## Q2. Shiny app

Develop a Shiny app for exploring the ICU cohort data created in Q1. The app should reside in the `mimiciv_shiny` folder. The app should contain at least two tabs. One tab provides easy access to the graphical and numerical summaries of variables (demographics, lab measurements, vitals) in the ICU cohort, using the `mimic_icu_cohort.rds` you curated in Q1. The other tab allows user to choose a specific patient in the cohort and display the patient's ADT and ICU stay information as we did in Q1 of HW3, by dynamically retrieving the patient's ADT and ICU stay information from BigQuery database. Again, do **not** ever add the BigQuery token to your Git repository. If you do so, you will lose 50 points.





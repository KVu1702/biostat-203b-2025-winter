---
title: "Biostat 203B Homework 1"
subtitle: Due Jan 24, 2024 @ 11:59PM
author: Your Name and UID
format:
  html:
    theme: cosmo
    embed-resources: true
    number-sections: false
    toc: true
    toc-depth: 4
    toc-location: left
    code-fold: false
    link-external-icon: true
    link-external-newwindow: true
---

Display machine information for reproducibility:
```{r}
#| eval: true
sessionInfo()
```

## Q1. Git/GitHub

**No handwritten homework reports are accepted for this course.** We work with Git and GitHub. Efficient and abundant use of Git, e.g., frequent and well-documented commits, is an important criterion for grading your homework.

1. Apply for the [Student Developer Pack](https://education.github.com/pack) at GitHub using your UCLA email. You'll get GitHub Pro account for free (unlimited public and private repositories).

2. Create a **private** repository `biostat-203b-2025-winter` and add `Hua-Zhou` and TA team (`Tomoki-Okuno` for Lec 1; `parsajamshidian` and `BowenZhang2001` for Lec 82) as your collaborators with write permission.

3. Top directories of the repository should be `hw1`, `hw2`, ... Maintain two branches `main` and `develop`. The `develop` branch will be your main playground, the place where you develop solution (code) to homework problems and write up report. The `main` branch will be your presentation area. Submit your homework files (Quarto file `qmd`, `html` file converted by Quarto, all code and extra data sets to reproduce results) in the `main` branch.

4. After each homework due date, course reader and instructor will check out your `main` branch for grading. Tag each of your homework submissions with tag names `hw1`, `hw2`, ... Tagging time will be used as your submission time. That means if you tag your `hw1` submission after deadline, penalty points will be deducted for late submission.

5. After this course, you can make this repository public and use it to demonstrate your skill sets on job market.

## Q2. Data ethics training

This exercise (and later in this course) uses the [MIMIC-IV data v3.1](https://physionet.org/content/mimiciv/3.1/), a freely accessible critical care database developed by the MIT Lab for Computational Physiology. Follow the instructions at <https://mimic.mit.edu/docs/gettingstarted/> to (1) complete the CITI `Data or Specimens Only Research` course and (2) obtain the PhysioNet credential for using the MIMIC-IV data. Display the verification links to your completion report and completion certificate here. **You must complete Q2 before working on the remaining questions.** (Hint: The CITI training takes a few hours and the PhysioNet credentialing takes a couple days; do not leave it to the last minute.)

**Solution:** 
CITI Conflicts of Interest Course - [Completetion Report](https://www.citiprogram.org/verify/?k0a45489b-60dd-4a66-bdee-455048a65e4e-67209991)/[Completetion Certificiate](https://www.citiprogram.org/verify/?wa8b11e53-e354-4bcd-ad32-cd259773ce85-67209991)
CITI Data or Specimens Only Research - [Completetion Report](https://www.citiprogram.org/verify/?kec805388-2cb9-4ae7-8ff2-137657525315-67209990)/[Completetion Certificiate](https://www.citiprogram.org/verify/?w80e757a3-49df-4796-a834-f5aa34e6db9d-67209990)

## Q3. Linux Shell Commands

1. Make the MIMIC-IV v3.1 data available at location `~/mimic`. The output of the `ls -l ~/mimic` command should be similar to the below (from my laptop).
```{bash}
#| eval: true
# content of mimic folder
ls -l ~/mimic/
```
Refer to the documentation <https://physionet.org/content/mimiciv/3.1/> for details of data files. Do **not** put these data files into Git; they are big. Do **not** copy them into your directory. Do **not** decompress the gz data files. These create unnecessary big files and are not big-data-friendly practices. Read from the data folder `~/mimic` directly in following exercises. 

  Use Bash commands to answer following questions.

2. Display the contents in the folders `hosp` and `icu` using Bash command `ls -l`. Why are these data files distributed as `.csv.gz` files instead of `.csv` (comma separated values) files? Read the page <https://mimic.mit.edu/docs/iv/> to understand what's in each folder.

**Solution:**
```{bash}
ls -l ~/mimic/hosp/
```
```{bash}
ls -l ~/mimic/icu/
```

These files are distributed as `.csv.gz` files because gzip compression ensures that the large data files in MIMIC-IV v3.1 are transferred efficiently between users.

3. Briefly describe what Bash commands `zcat`, `zless`, `zmore`, and `zgrep` do.

**Solution:**
The `z` in front of Bash commands such as `cat`, `less`, `more`, and `grep` means that the file is uncompressed before the Bash command is executed. 
`zcat`: Uncompresses the file before printing its contents.

`zless`: Uncompresses the file and browses a text file by screen through actions such as upwards and downward scrolling.

`zmore`: Uncompresses the file and also acts as a pager. `more` has less functionality than `less`, only allowing downward scrolling. `more` reads the whole file, making it slower in reading files than `less`. Exit `more` by pressing the q key and scroll/page using the spacebar.

`zgrep`: Uncompresses a file and prints lines matching an expression.

4. (Looping in Bash) What's the output of the following bash script?
```{bash}
#| eval: false
for datafile in ~/mimic/hosp/{a,l,pa}*.gz
do
  ls -l $datafile
done
```

Display the number of lines in each data file using a similar loop. (Hint: combine linux commands `zcat <` and `wc -l`.)

**Solution:**
The output of the above Bash script prints all files in /mimic/hosp/ that starts with a, l, or pa.

```{bash}
#| eval: false
for datafile in ~/mimic/hosp/{a,l,pa}*.gz
do
  # For each file that starts with a, l, or pa, count the number of lines with wc and the -l option for lines.
  zcat < $datafile | wc -l
done
```

5. Display the first few lines of `admissions.csv.gz`. How many rows are in this data file, excluding the header line? Each `hadm_id` identifies a hospitalization. How many hospitalizations are in this data file? How many unique patients (identified by `subject_id`) are in this data file? Do they match the number of patients listed in the `patients.csv.gz` file? (Hint: combine Linux commands `zcat <`, `head`/`tail`, `awk`, `sort`, `uniq`, `wc`, and so on.)

**Solution:**
```{bash}

# Printing the first 10 rows in admissions.csv.gz
echo 'The first ten lines of admissions.csv.gz are:'
zcat < ~/mimic/hosp/admissions.csv.gz | head -10
echo ''

# Counting the number of lines minus the header using wc -l.
# The n- option for tail displays the lines beginning from the n-th line.

echo 'The total count of lines in admissions.csv.gz, minus the header, are:'
zcat < ~/mimic/hosp/admissions.csv.gz | tail -n +2 | wc -l
echo ''

# Counting the number of unique values in the second column, hadm_id, minus the header, using the Bash command awk.
# awk scans files line by line. The -F option in awk determines the delimiter for the file, and {print $2} prints the value after the second delimiter, representing the second column.
# For a .csv file, this delimiter is a ','
# sort is a bash command that sorts values. We sort because the uniq bash command only removes and identifies duplicates directly adjacent.
# The bash command uniq with option -c counts the occurrences.
# We sort again using -n, an option that sorts numerically, and -r, an option for reverse order (descending instead of ascending).
# Using wc -l to count the number of rows counted by uniq. Repeated values are compressed to one line, a count followed by the counted instance.

echo 'The total count of hadm_id in admissions.csv.gz, minus the header, are:'
zcat < ~/mimic/hosp/admissions.csv.gz | tail -n +2 | awk -F ',' '{print $2}' | sort | uniq -c | sort -nr | wc -l
echo ''

#Counting the number of unique patients/subject_id, the first column in admissions.csv.gz.
echo 'The total count of unique patients by subject_id in admissions.csv.gz, minus the header, are:'
zcat < ~/mimic/hosp/admissions.csv.gz | tail -n +2 | awk -F ',' '{print $1}' | sort | uniq -c | sort -nr | wc -l
echo ''

#subject_id is the first column in patients.csv.gz.
echo 'The total count of unique patients by subject_id in patients.csv.gz, minus the header, are:'
zcat < ~/mimic/hosp/patients.csv.gz | tail -n +2 | awk -F ',' '{print $1}' | sort | uniq -c | sort -nr | wc -l
echo ''

```

We find that there are 546028 rows in `admissions.csv.gz` minus the header file. Of those 546028, 223452 unique patients are hospitalized. However, there are 364627 patients in `patients.csv.gz.`, meaning that some patients did not get hospitalized.

6. What are the possible values taken by each of the variable `admission_type`, `admission_location`, `insurance`, and `ethnicity`? Also report the count for each unique value of these variables in decreasing order. (Hint: combine Linux commands `zcat`, `head`/`tail`, `awk`, `uniq -c`, `wc`, `sort`, and so on; skip the header line.)

**Solution:**

```{bash}

#admission_type is the sixth column in admissions.csv.gz.
echo 'The possible values for admission types and their frequencies are:'
zcat < ~/mimic/hosp/admissions.csv.gz | tail -n +2 | awk -F ',' '{print $6}' | sort | uniq -c | sort -nr
echo ''

#admission_location is the eighth column in admissions.csv.gz.
echo 'The possible values for admission location and their frequencies are:'
zcat < ~/mimic/hosp/admissions.csv.gz | tail -n +2 | awk -F ',' '{print $8}' | sort | uniq -c | sort -nr
echo ''

#insurance is the tenth column in admissions.csv.gz.
echo 'The possible values for insurance and their frequencies are:'
zcat < ~/mimic/hosp/admissions.csv.gz | tail -n +2 | awk -F ',' '{print $10}' | sort | uniq -c | sort -nr
echo ''

#ethnicity/race is the tenth column in admissions.csv.gz.
echo 'The possible values for ethnicity/race and their frequencies are:'
zcat < ~/mimic/hosp/admissions.csv.gz | tail -n +2 | awk -F ',' '{print $13}' | sort | uniq -c | sort -nr
echo ''

```

7. The `icustays.csv.gz` file contains all the ICU stays during the study period. How many ICU stays, identified by `stay_id`, are in this data file? How many unique patients, identified by `subject_id`, are in this data file?

**Solution:**
```{bash}
# Printing the first 5 rows in icustays.csv.gz.
echo 'The first 5 lines of icustays.csv.gz are:'
zcat < ~/mimic/icu/icustays.csv.gz | head -5
echo ''

#stay_id is the third column of icustays.csv.gz.
echo 'The total count of ICU stays by stay_id in icustays.csv.gz, minus the header, are:'
zcat < ~/mimic/icu/icustays.csv.gz | tail -n +2 | awk -F ',' '{print $3}' | sort | uniq -c | sort -nr | wc -l
echo ''

#subject_id is the third column of icustays.csv.gz
echo 'The total count of unique patients by subject_id in icustays.csv.gz, minus the header, are:'
zcat < ~/mimic/icu/icustays.csv.gz | tail -n +2 | awk -F ',' '{print $1}' | sort | uniq -c | sort -nr | wc -l
echo ''

```

8. _To compress, or not to compress. That's the question._ Let's focus on the big data file `labevents.csv.gz`. Compare compressed gz file size to the uncompressed file size. Compare the run times of `zcat < ~/mimic/labevents.csv.gz | wc -l` versus `wc -l labevents.csv`. Discuss the trade off between storage and speed for big data files. (Hint: `gzip -dk < FILENAME.gz > ./FILENAME`. Remember to delete the large `labevents.csv` file after the exercise.)

**Solution:**
```{bash}

#Running wc on the compressed file labevents.csv.gz.
zcat < ~/mimic/hosp/labevents.csv.gz | wc -l

#Unzipping the compressed file labevents.csv.gz.
gzip -dk ~/mimic/hosp/labevents.csv.gz ./labevents

#Running wc on the uncompressed file labevents.csv
wc -l ~/mimic/hosp/labevents.csv

#Deleting the file afterwards
rm ~/mimic/hosp/labevents.csv
```

The tradeoff between using compressed versus uncompressed files is that they take up much less storage space than their compressed counterparts. However, it is faster to access the raw, large, uncompressed files versus their uncompressed counterparts because you must first uncompress the compressed file before accessing it. The uncompression algorithm can be lengthy depending on your computer specifications, file size, and software.

## Q4. Who's popular in Price and Prejudice

1. You and your friend just have finished reading *Pride and Prejudice* by Jane Austen. Among the four main characters in the book, Elizabeth, Jane, Lydia, and Darcy, your friend thinks that Darcy was the most mentioned. You, however, are certain it was Elizabeth. Obtain the full text of the novel from <http://www.gutenberg.org/cache/epub/42671/pg42671.txt> and save to your local folder. 
```{bash}
#| eval: false
wget -nc http://www.gutenberg.org/cache/epub/42671/pg42671.txt
```
Explain what `wget -nc` does. Do **not** put this text file `pg42671.txt` in Git. Complete the following loop to tabulate the number of times each of the four characters is mentioned using Linux commands.
```{bash}
#| eval: false
wget -nc http://www.gutenberg.org/cache/epub/42671/pg42671.txt
for char in Elizabeth Jane Lydia Darcy
do
  echo $char:
  grep -o $char pg42671.txt | wc -l
done
```

2. What's the difference between the following two commands?
```{bash}
#| eval: false
echo 'hello, world' > test1.txt
```
and
```{bash}
#| eval: false
echo 'hello, world' >> test2.txt
```

3. Using your favorite text editor (e.g., `vi`), type the following and save the file as `middle.sh`:
```{bash eval=FALSE}
#!/bin/sh
# Select lines from the middle of a file.
# Usage: bash middle.sh filename end_line num_lines
head -n "$2" "$1" | tail -n "$3"
```
Using `chmod` to make the file executable by the owner, and run
```{bash}
#| eval: false
./middle.sh pg42671.txt 20 5
```
Explain the output. Explain the meaning of `"$1"`, `"$2"`, and `"$3"` in this shell script. Why do we need the first line of the shell script?

## Q5. More fun with Linux

Try following commands in Bash and interpret the results: `cal`, `cal 2025`, `cal 9 1752` (anything unusual?), `date`, `hostname`, `arch`, `uname -a`, `uptime`, `who am i`, `who`, `w`, `id`, `last | head`, `echo {con,pre}{sent,fer}{s,ed}`, `time sleep 5`, `history | tail`.

## Q6. Book

1. Git clone the repository <https://github.com/christophergandrud/Rep-Res-Book> for the book _Reproducible Research with R and RStudio_ to your local machine. Do **not** put this repository within your homework repository `biostat-203b-2025-winter`.

2. Open the project by clicking `rep-res-3rd-edition.Rproj` and compile the book by clicking `Build Book` in the `Build` panel of RStudio. (Hint: I was able to build `git_book` and `epub_book` directly. For `pdf_book`, I needed to add a line `\usepackage{hyperref}` to the file `Rep-Res-Book/rep-res-3rd-edition/latex/preabmle.tex`.)

The point of this exercise is (1) to obtain the book for free and (2) to see an example how a complicated project such as a book can be organized in a reproducible way. Use `sudo apt install PKGNAME` to install required Ubuntu packages and `tlmgr install PKGNAME` to install missing TexLive packages.

For grading purpose, include a screenshot of Section 4.1.5 of the book here.

![Section 4.1.5 of Reproducible Research with R and RStudio](~/biostat-203b-hw/hw1/rep-res-3rd-edition.png)

